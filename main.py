import streamlit as st
import pandas as pd
import os
from pathlib import Path

st.set_page_config(page_title="Urantia Theme Study (alpha)", layout="wide")

DATA_DIR = Path("data")
GLOSSARY_CANDIDATES = [
    DATA_DIR / "English_Master_Glossary.xlsx",
    DATA_DIR / "glossary.xlsx",
]
EN_PATH = DATA_DIR / "urantia_en.txt"

# ---------------------------
# í—¬í¼: í…ìŠ¤íŠ¸ ì•ˆì „í•˜ê²Œ ì½ê¸°
# ---------------------------
def safe_read_text(path: Path):
    if not path.exists():
        return "", 0
    encodings = ["utf-8", "utf-8-sig", "cp949", "euc-kr", "latin-1"]
    for enc in encodings:
        try:
            text = path.read_text(encoding=enc)
            return text, len(text.splitlines())
        except Exception:
            continue
    # ë§ˆì§€ë§‰ ìˆ˜ë‹¨
    text = path.read_text(encoding="utf-8", errors="replace")
    return text, len(text.splitlines())

# ---------------------------
# í—¬í¼: glossary ì½ê¸°
# ---------------------------
@st.cache_data
def load_glossary():
    for cand in GLOSSARY_CANDIDATES:
        if cand.exists():
            try:
                df = pd.read_excel(cand)
                # ì»¬ëŸ¼ ì´ë¦„ì„ ì†Œë¬¸ìë¡œ
                df.columns = [str(c).strip().lower() for c in df.columns]
                return df, cand.name
            except Exception as e:
                return None, f"{cand.name} ì½ê¸° ì‹¤íŒ¨: {e}"
    return None, "glossary íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

# ---------------------------
# ì‹¤ì œ ë°ì´í„° ì½ê¸°
# ---------------------------
glossary_df, glossary_status = load_glossary()
en_text, en_lines = safe_read_text(EN_PATH)

st.title("ğŸ“˜ Urantia Theme Study (alpha)")
st.caption("Keyword â†’ glossary â†’ source passages â†’ AI (ë‚˜ì¤‘ì—)")

# ë””ë²„ê·¸ ì •ë³´ (ì§€ê¸ˆì€ ë³´ì´ê²Œ í•´ë‘ )
with st.expander("ğŸ“¦ Data status (ì´ê±´ ì„ì‹œë¡œ ë³´ì´ê²Œ í•©ë‹ˆë‹¤)", expanded=True):
    st.write(f"ğŸ“ data/ ë””ë ‰í† ë¦¬ ì¡´ì¬: {DATA_DIR.exists()}")
    st.write(f"ğŸ“„ urantia_en.txt ì¡´ì¬: {EN_PATH.exists()} (lines: {en_lines})")
    st.write(f"ğŸ“„ glossary ìƒíƒœ: {glossary_status}")

term = st.text_input("ğŸ” ì£¼ì œ / ìš©ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: Thought Adjuster, faith, Michael)", "")

if term:
    term_low = term.lower().strip()

    # 1. Glossary lookup
    st.subheader("1. Glossary lookup")
term = st.text_input("ì°¾ê³  ì‹¶ì€ ìš©ì–´ (ì˜ì–´ ë˜ëŠ” í•œêµ­ì–´):", "", key="glossary_input").strip()

if glossary is not None and term:
    df = glossary.copy()
    # âœ… ì»¬ëŸ¼ ì´ë¦„ì„ ì†Œë¬¸ì, ê³µë°± ì œê±°
    df.columns = [c.strip().lower() for c in df.columns]
    # âœ… term / definition ì»¬ëŸ¼ ê°•ì œ ë³´ì •
    if "term" not in df.columns:
        for alt in ["word", "entry", "expression"]:
            if alt in df.columns:
                df.rename(columns={alt: "term"}, inplace=True)
    if "definition" not in df.columns:
        for alt in ["description", "meaning", "explanation"]:
            if alt in df.columns:
                df.rename(columns={alt: "definition"}, inplace=True)
    # âœ… ê²€ìƒ‰ ì²˜ë¦¬
    df["term"] = df["term"].astype(str).str.strip().str.lower()
    df["definition"] = df["definition"].astype(str)
    found = df[df["term"].str.contains(term.lower(), case=False, na=False)]
    # âœ… ì¶œë ¥
    if len(found) > 0:
        for _, row in found.iterrows():
            st.markdown(f"**{row['term'].capitalize()}** â€” {row['definition']}")
    else:
        st.info("No glossary match found for this term.")

        else:
            for _, row in hits.iterrows():
                st.markdown("---")
                # ì œëª© í›„ë³´
                title = None
                if possible_term_cols:
                    for c in possible_term_cols:
                        if c in row:
                            title = row[c]
                            break
                if not title:
                    title = term
                st.markdown(f"**ğŸ”¹ {title}**")
                # ì„¤ëª… í›„ë³´
                body = None
                if possible_def_cols:
                    for c in possible_def_cols:
                        if c in row and row[c] not in ["", "nan", "None"]:
                            body = row[c]
                            break
                if not body:
                    # ë‚¨ëŠ” ì»¬ëŸ¼ í•©ì³ì„œ
                    body_parts = []
                    for c in df.columns:
                        val = row.get(c, "")
                        if isinstance(val, str) and val not in ["", "nan", "None"]:
                            body_parts.append(f"**{c}**: {val}")
                    body = "\n\n".join(body_parts)
                st.write(body)

    # 2. Passages in The Urantia Book
    st.subheader("2. Passages in The Urantia Book")
    if not en_text:
        st.warning("urantia_en.txt ë¥¼ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. data/ ì•ˆì— ìˆê³  UTF-8 ë˜ëŠ” UTF-8-SIG ë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    else:
        # ì¤„ ë‹¨ìœ„ë¡œ ê²€ìƒ‰
        lines = en_text.splitlines()
        hits = []
        for line in lines:
            if term_low in line.lower():
                hits.append(line.strip())
        if not hits:
            st.info("No passages found in urantia_en.txt containing that keyword.")
        else:
            st.markdown(f"**Found {len(hits)} passages containing '{term}':**")
            for h in hits[:50]:
                st.markdown(f"- {h}")

    # 3. Topic importance check (í˜•ì‹ë§Œ)
    st.subheader("3. Topic importance check")
    st.write("ì´ ì£¼ì œê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ì• ë§¤í•˜ë©´ AI ì„¤ëª…ì„ ê±´ë„ˆë›°ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§€ê¸ˆì€ ìˆ˜ë™ ëª¨ë“œì…ë‹ˆë‹¤.")

    # 4. AI study material (í˜„ì¬ëŠ” ìë¦¬ë§Œ)
    st.subheader("4. AI study material")
    st.write("í˜„ì¬ëŠ” OpenAI í˜¸ì¶œ ë¶€ë¶„ì„ ë¹„ì›Œë‘ì—ˆìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ë³¸ë¬¸ì´ 1ê°œ ì´ìƒì´ë¼ë©´ ì—¬ê¸°ì„œ GPT í˜¸ì¶œì„ ë¶™ì´ë©´ ë©ë‹ˆë‹¤.")
else:
    st.info("ë¨¼ì € ìœ„ ì…ë ¥ì°½ì— ì°¾ê³  ì‹¶ì€ ì£¼ì œë‚˜ ë‹¨ì–´ë¥¼ ë„£ì–´ ì£¼ì„¸ìš”.")






